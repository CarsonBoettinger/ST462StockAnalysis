---
title: "stock analysis"
output:
  pdf_document: default
  html_document: default
editor_options:
  markdown:
    wrap: 120
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(error = TRUE)
knitr::opts_chunk$set(fig.show = 'asis')
knitr::opts_chunk$set(fig.width = 7, fig.height = 7)



# Import libraries
library(gridExtra)
library(knitr)
library(kableExtra)
library(lme4)
library(ICC)
library(knitr)
library(tidyverse)
library(tidyquant)
library(xts)
library(PerformanceAnalytics)
library(corrplot)
library(dplyr)
library(Matrix)


# Import the CSV file
df <- read.csv("stockdataformatted.csv")
fulldf <- na.omit(df)

# Identify missing values
sapply(df, function(x) sum(is.na(x)))

# Split the dataframe by the 'Company.Name' column
list_of_companies <- split(df, f = df$Company.Name)

summary(df)


# Initialize a list to hold correlation results for each company
correlations_by_company <- list()




```


```{r}
# Select only the numeric columns
numeric_df <- df %>% select_if(is.numeric)

# Calculate the correlation matrix
cor_matrix <- cor(numeric_df, use = "complete.obs")

# Visualize the correlation matrix
corrplot(cor_matrix, method = "circle")
```
This is likely inaccurate due to a repeating of quarters for every company

```{r}

# Code below gets the correlations for the first 10 companies between last sale price and the rest of the variables


# Get the names of the first 10 companies from the list
first_10_companies <- names(list_of_companies)[1:10]

# Loop through each of the first 10 companies
for(n in first_10_companies) {
  # Select the current company's dataframe
  df <- list_of_companies[[n]]

  # Determine the third-last variable name
  third_last_var <- names(df)[ncol(df)-2]

  # Select only the numeric columns and the third-last variable
  numeric_df <- df %>% select_if(is.numeric) %>% select(third_last_var, everything())

  # Initialize a vector to store the correlations
  cor_vector <- numeric(ncol(numeric_df))
  names(cor_vector) <- names(numeric_df)

  # Calculate the correlation of the third-last variable with all others
  for(column_name in names(numeric_df)) {
    if(sum(complete.cases(numeric_df[[third_last_var]], numeric_df[[column_name]])) > 1) {
      # Compute the correlation if there are enough complete cases
      cor_vector[column_name] <- cor(numeric_df[[third_last_var]], numeric_df[[column_name]], use = "complete.obs")
    } else {
      # Assign NA if not enough complete cases
      cor_vector[column_name] <- NA
    }
  }

  # Check if there are any non-NA values to plot
  if(all(is.na(cor_vector))) {
    cat(paste("Correlation plot for", n, "cannot be generated due to insufficient data.\n"))
  } else {
    # Construct a matrix from the vector for plotting
    cor_matrix <- matrix(cor_vector, nrow = 1, byrow = TRUE)
    colnames(cor_matrix) <- names(cor_vector)
    rownames(cor_matrix) <- third_last_var

    # Plot title with the company name for clarity
    plot_title <- paste("Correlation with", third_last_var, "for", n)

    # Visualize the correlation matrix using corrplot
    print(corrplot(cor_matrix, method = "circle", title = plot_title, is.corr = FALSE))
  }
}
```
As you can see here each company depends on different Level 1 factors which affect its price

```{r}
# Code gets the mean correlation matrix between last sale price and all other variables


# Initialize a list to store the correlation data for each company
cor_data_list <- list()

# Get the names of the first 500 companies from the list
all_companies <- names(list_of_companies)

# Utility function to check if there's sufficient data to compute the standard deviation
is_var_constant <- function(x) {
  length(unique(na.omit(x))) <= 1
}

# Loop through each of the companies
for(n in all_companies) {
  # Check if the company's data frame exists and is not NULL
  if (!is.null(list_of_companies[[n]]) && ncol(list_of_companies[[n]]) > 2) {
    # Select the current company's dataframe
    df <- list_of_companies[[n]]

    # Determine the third-last variable name
    third_last_var <- names(df)[ncol(df)-2]

    # Select only the numeric columns
    numeric_df <- df %>% select_if(is.numeric)

    # Initialize a vector to store the correlations for the current company
    cor_vector <- rep(NA, ncol(numeric_df))

    # Calculate the correlation of the third-last variable with all other numeric variables
    for(i in seq_along(numeric_df)) {
      # Check if the variable is constant (no variance)
      if (!is_var_constant(numeric_df[[third_last_var]]) && !is_var_constant(numeric_df[[i]])) {
        cor_vector[i] <- cor(numeric_df[[third_last_var]], numeric_df[[i]], use = "complete.obs")
      }
      # Else, the result is already NA
    }

    # Name the elements of the vector with the column names
    names(cor_vector) <- names(numeric_df)

    # Store the results in the cor_data_list
    cor_data_list[[n]] <- cor_vector
  } else {
    # Print a message if the company's data frame does not exist or is NULL
    cat(paste("The data frame for company", n, "does not exist or is NULL.\n"))
  }
}



  # Name the elements of the vector with the column names
  names(cor_vector) <- names(numeric_df)

  # Store the results in the cor_data_list
  cor_data_list[[n]] <- cor_vector

# Combine all the correlation vectors into one data frame
cor_data_frame <- do.call(rbind, cor_data_list)

# Compute the mean of the correlation coefficients for the third-last variable across all companies
mean_correlation <- colMeans(cor_data_frame, na.rm = TRUE)

# Output the mean correlation as a single row data frame
mean_correlation_df <- as.data.frame(t(mean_correlation))

head(mean_correlation_df)

corrplot(as.matrix(mean_correlation_df), is.corr=FALSE)
```
On average most variables are correlated with price, but many of these variables are also correlated with each other

```{r}
# Calculating the mean full correlation matrix


# Initialize a list to store the full correlation matrix for each company
full_cor_matrices <- list()
common_columns <- NULL


# Loop through each company
for(n in all_companies) {
  df <- list_of_companies[[n]]
  if (!is.null(df) && ncol(df) > 2) {
    numeric_df <- df %>%
      select_if(is.numeric) %>%
      na.omit()

    # Check if there's enough variation in each column to calculate correlation
    numeric_df <- numeric_df %>%
      select_if(~sd(.) != 0)

    if(nrow(numeric_df) >= 2) {
      cor_matrix <- cor(numeric_df)
      full_cor_matrices[[n]] <- cor_matrix
      
      # Identify common columns if not already identified
      if (is.null(common_columns)) {
        common_columns <- names(numeric_df)
      } else {
        common_columns <- intersect(common_columns, names(numeric_df))
      }
    }
  }
}

# Function to compute the mean correlation matrix from the list of matrices
compute_mean_cor_matrix <- function(cor_matrices, common_columns) {
  # Subset matrices to only common columns
  cor_matrices <- lapply(cor_matrices, function(m) m[common_columns, common_columns])
  
  # Calculate mean correlation matrix using Reduce to sum the matrices
  sum_matrix <- Reduce("+", cor_matrices)
  counts <- Reduce("+", lapply(cor_matrices, function(m) !is.na(m)))
  
  # Calculate the mean matrix by dividing sum by counts
  mean_matrix <- sum_matrix / counts
  
  # Replace NaN values with NA
  mean_matrix[is.nan(mean_matrix)] <- NA
  return(mean_matrix)
}

# Ensure we have correlation matrices and common columns to work with
if(length(full_cor_matrices) > 0 && !is.null(common_columns)) {
  mean_cor_matrix <- compute_mean_cor_matrix(full_cor_matrices, common_columns)
  
  # Visualize the mean correlation matrix
  corrplot(mean_cor_matrix, method = "circle", title = "Mean Correlation Matrix Across All Companies")
} else {
  cat("No correlation matrices available to compute the mean matrix.\n")
}


head(mean_cor_matrix)

```
This is way more accurate than the full correlation matrix from earlier

```{r}

# Does the same thing as the mean correlation matrix between last sale price and all other variables but groups by industry (level 2)

# Initialize a list to store the correlation data for each company
cor_data_list <- list()

# Get the names of the first 500 companies from the list
all_companies <- names(list_of_companies)

# Utility function to check if there's sufficient data to compute the standard deviation
is_var_constant <- function(x) {
  length(unique(na.omit(x))) <= 1
}

# Loop through each of the companies
for(n in all_companies) {
  # Check if the company's data frame exists and is not NULL
  if (!is.null(list_of_companies[[n]]) && ncol(list_of_companies[[n]]) > 2) {
    # Select the current company's dataframe
    df <- list_of_companies[[n]]

    # Determine the third-last variable name
    third_last_var <- names(df)[ncol(df)-2]

    # Select only the numeric columns
    numeric_df <- df %>% select_if(is.numeric)

    # Initialize a vector to store the correlations for the current company
    cor_vector <- rep(NA, ncol(numeric_df))

    # Calculate the correlation of the third-last variable with all other numeric variables
    for(i in seq_along(numeric_df)) {
      # Check if the variable is constant (no variance)
      if (!is_var_constant(numeric_df[[third_last_var]]) && !is_var_constant(numeric_df[[i]])) {
        cor_vector[i] <- cor(numeric_df[[third_last_var]], numeric_df[[i]], use = "complete.obs")
      }
      
      # Else, the result is already NA
    }

    # Name the elements of the vector with the column names
    names(cor_vector) <- names(numeric_df)
    
    # Inside the loop, after calculating cor_vector for each company:
    cor_vector['Primary.Industry'] <- df$Primary.Industry[1]


    # Store the results in the cor_data_list
    cor_data_list[[n]] <- cor_vector
  } else {
    # Print a message if the company's data frame does not exist or is NULL
    cat(paste("The data frame for company", n, "does not exist or is NULL.\n"))
  }
}


  # Name the elements of the vector with the column names
  names(cor_vector) <- names(numeric_df)

  # Store the results in the cor_data_list
  cor_data_list[[n]] <- cor_vector
  
# Convert cor_data_list to a data frame
cor_data_frame_with_industry <- do.call(rbind, cor_data_list)

# Convert to a proper data frame and handle factor conversion for the 'Primary.Industry' column
cor_data_frame_with_industry <- data.frame(cor_data_frame_with_industry, stringsAsFactors = FALSE)
cor_data_frame_with_industry$Primary.Industry <- as.factor(cor_data_frame_with_industry$Primary.Industry)

# Calculate the mean correlation by industry
mean_correlation_by_industry <- cor_data_frame_with_industry %>%
  group_by(Primary.Industry)


library(dplyr)
library(tibble)

industry_filtered <- na.omit(mean_correlation_by_industry) 


num_cols <- ncol(industry_filtered) - 1  # Get the number of columns minus 1
industry_filtered[, 1:num_cols] <- lapply(industry_filtered[, 1:num_cols], as.numeric) 

industry_correlations <- industry_filtered %>%
  group_by(Primary.Industry) %>%
  summarize(across(where(is.numeric), mean))

head(industry_correlations, 10)

```
Depending on the industry, each variable can be strongly or negatively correlated with last sale price


Model Building

Level 1 - Individual time period observations

Every level 1 variable has a strong correlation with at least one industry, so it's best to leave all variables in the model other than Year, but highly intercorrelated variable pairs such as EBITDA and EBITDA margin you can just select one of them

Selected Variables - Last Sale Price Change from Baseline (response), Revenue, EBITDA, Normalized Net Income, Gross Profit Margin, EBITDA Margin, Normalized Net Income Margin, Debt to Equity, Current Ratio, CPI, Unemployment Rate, Fed Funds Rate, Book Value per Share

Strong intercorrelation with revenue, ebitda, normalized net income, and their margins

Debt/Equity negatively correlated with book value per share

CPI and Fed Funds negatively correlated with Unemployment rate but not correlated to each other 



Level 2 - Company (Grouping Factor)

Primary.Industry and Company Name are the only level 2 variables

All level 1 variables other than CPI, Fed Funds, Unemployment Rate vary based on Company Name

The company heavily affects the weighting of each variable on the response

The industry also affects the weighting of each variable on the response, but the company will be the most accurate, so maybe industry should be dropped


```{r}
# Model A (Unconditinal means model)

model.a <- lmer(fulldf$Last.Sale.Price.Change.From.Baseline.... ~ 1 + (1|fulldf$Company.Name), REML = T, data = fulldf)

summary(model.a)

```
Looking at the random effects, the variation of 8070 suggests that there are large differences in variation between companies when it comes to their stock price movements. The residual variance of 5358 shows the deviation of the stock prices for the same company for all the quarters compared to the mean baseline change of the same school.

Intercept of 40.309 suggests that the average stock price level between Q1 2019 and Q4 2023 was 1.4x its baseline value.

~60% of total variation in stock price levels is attributable to differences between companies rather than changes over time in the same company.

```{r}
# Model B (Unconditional growth)

head(fulldf)

# Period.Ending needs to be converted into a numeric variable

# Extract year and quarter
fulldf$Year <- as.numeric(substr(fulldf$Period.Ending, 4, 7))
fulldf$Quarter <- as.numeric(substr(fulldf$Period.Ending, 3, 3))

# Calculate the numeric representation
# Assuming FQ12019 is the first quarter (0) and incrementing by 1 for each subsequent quarter
fulldf$Period.Numeric <- ((fulldf$Year - 2019) * 4) + (fulldf$Quarter - 1)


model.b <- lmer(fulldf$Last.Sale.Price.Change.From.Baseline.... ~ fulldf$Period.Numeric + (fulldf$Period.Numeric|fulldf$Company.Name), REML=T, data= fulldf)

summary(model.b)


```
625.61 represents the variance between companies in baseline price (baseline price should be constant for all companies but some have incomplete data due to NA values hence the extreme variance)

This also applies to the estimate of the intercept being 1.3661 instead of zero

63.84 represents the variance between companies in rates of change in stock prices during the 20 quarter period

2598.21 is the variance within companies which makes sense which between 2019 and 2023 prices can change greatly

```{r}
anova(model.a, model.b)
```
```{r}
# Model C (adds industry to the level 2 variables)

model.c <- lmer(Last.Sale.Price.Change.From.Baseline.... ~ Period.Numeric + as.factor(Primary.Industry):Period.Numeric + (Period.Numeric|Company.Name), REML=T, data= fulldf)

summary(model.c)

anova(model.b, model.c)

```
As you can see here, adding the primary industry to the model provides a wide range of estimate stock price mean increases/decreases over the course of the 4 years. However, this complicates the model a lot and results in higher AIC and BIC scores, so industry will be dropped.

```{r}
# Model D - adds all the chosen level 1 variables

# Selected Variables - Last Sale Price Change from Baseline (response), Revenue, EBITDA, Normalized Net Income, Gross Profit Margin, EBITDA Margin, Normalized Net Income Margin, Debt to Equity, Current Ratio, CPI, Unemployment Rate, Fed Funds Rate, Book Value per Share

# Strong intercorrelation with revenue, ebitda, normalized net income, and their margins

# Debt/Equity negatively correlated with book value per share

# CPI and Fed Funds negatively correlated with Unemployment rate but not correlated with eachother 

head(fulldf)

# model.d <- lmer(Last.Sale.Price.Change.From.Baseline.... ~ Period.Numeric + Revenue...M. + EBITDA...M. + Normalized.Net.Income...M. + Gross.Profit.Margin + EBITDA.Margin + Normalized.Net.Income.Margin + Debt.to.Equity + Current.Ratio + US.YOY.CPI + Unemployment.Rate + Fed.Funds.Rate + Book.Value.Share + (Period.Numeric + Revenue...M. + EBITDA...M. + Normalized.Net.Income...M. + Gross.Profit.Margin + EBITDA.Margin + Normalized.Net.Income.Margin + Debt.to.Equity + Current.Ratio + US.YOY.CPI + Unemployment.Rate + Fed.Funds.Rate + Book.Value.Share|Company.Name), REML=T, data= fulldf)

# Fitting the model resulted in infinite loading screen, try smaller model

# Interaction Terms
# Revenue...M.*EBITDA...M.*Normalized.Net.Income...M.*Gross.Profit.Margin*EBITDA.Margin*Normalized.Net.Income.Margin
# Unemployment.Rate*(US.YOY.CPI + Fed.Funds.Rate)

# Rescale all variables

library(dplyr)

fulldf <- fulldf %>%
  group_by(Company.Name) %>%
  mutate(Last_Price_M_Scaled = `Last.Sale.Price` / max(`Last.Sale.Price`, na.rm = TRUE),
         Revenue_M_Scaled = `Revenue...M.` / max(`Revenue...M.`, na.rm = TRUE),
         EBITDA_M_Scaled = `EBITDA...M.` / max(`EBITDA...M.`, na.rm = TRUE),
         Normalized_Net_Income_M_Scaled = `Normalized.Net.Income...M.` / max(`Normalized.Net.Income...M.`, na.rm = TRUE),
         Book_Value_Share_Scaled = `Book.Value.Share` / max(`Book.Value.Share`, na.rm = TRUE),
         Debt_to_Equity_Scaled = `Debt.to.Equity` / max(`Debt.to.Equity`, na.rm = TRUE)) %>%
  ungroup()

head(fulldf, 10)

# Try Again

model.d <- lmer(Last.Sale.Price.Change.From.Baseline.... ~ Period.Numeric + Revenue_M_Scaled + (Period.Numeric + Revenue_M_Scaled|Company.Name), REML=T, data= fulldf)

summary(model.d)

```
This makes more sense,

Random Effects
2047.18 -> variance in baseline price between companies (as all should be 0 to start)
31.26 -> variance in slope for price changes between companies
14907.96 -> variance in how revenue affects price across different companies, indicating that for some companies revenue may have a huge impact whereas for others none at all

Fixed Effects
-34.0342 -> mean baseline price when all predictors are 0, it goes negative because revenue is never zero
2.7372 -> over time stock prices went up during this time, all else being constant
67.5486 -> an addition of a company's max revenue during the time period will increase the price 67.5486 over its baseline

```{r}
anova(model.b, model.d)

```
Lower AIC BIC for model.d which is a good sign

```{r}
# Model E - adding more and more variables

model.e <- lmer(Last.Sale.Price.Change.From.Baseline.... ~ Period.Numeric + Revenue_M_Scaled + Normalized_Net_Income_M_Scaled + Revenue_M_Scaled:Normalized_Net_Income_M_Scaled + Debt_to_Equity_Scaled + Current.Ratio + US.YOY.CPI + Fed.Funds.Rate + (Period.Numeric + Revenue_M_Scaled + Normalized_Net_Income_M_Scaled + Debt_to_Equity_Scaled + Current.Ratio + US.YOY.CPI + Fed.Funds.Rate|Company.Name), REML=T, data= fulldf)

# This model takes forever to fit therefore leaving out any level 1 variable that is correlated with any other level 1 variable, but leaving in normalized net income 

summary(model.e)

```
```{r}
anova(model.d, model.e)
```
Model E's AIC and BIC is significantly lower than D's thus demonstrating a better fit, but some variables can be removed

```{r}
# Model F - removing the interaction factor and current ratio

model.f <- lmer(Last.Sale.Price.Change.From.Baseline.... ~ Period.Numeric + Revenue_M_Scaled + Normalized_Net_Income_M_Scaled + Debt_to_Equity_Scaled + US.YOY.CPI + Fed.Funds.Rate + (Period.Numeric + Revenue_M_Scaled + Normalized_Net_Income_M_Scaled + Debt_to_Equity_Scaled + US.YOY.CPI + Fed.Funds.Rate|Company.Name), REML=T, data= fulldf)

summary(model.f)
```

```{r}
anova(model.e, model.f)

```
Model f is an improvement over e, although small

```{r}
# Model G - dropping CPI, adding Book Value/Share and Unemployment Rate

model.g <- lmer(Last.Sale.Price.Change.From.Baseline.... ~ Period.Numeric + Revenue_M_Scaled + Normalized_Net_Income_M_Scaled + Book_Value_Share_Scaled + Debt_to_Equity_Scaled + Fed.Funds.Rate + Unemployment.Rate + (Period.Numeric + Revenue_M_Scaled + Normalized_Net_Income_M_Scaled + Book_Value_Share_Scaled + Debt_to_Equity_Scaled + Fed.Funds.Rate + Unemployment.Rate|Company.Name), REML=T, data= fulldf)

summary(model.g)
```
```{r}
anova(model.f, model.g)

```
f outperformed g surprisingly

```{r}
# Model H - dropping BV/share and adding interaction effects

# model.h <- lmer(Last.Sale.Price.Change.From.Baseline.... ~ Period.Numeric*(Revenue_M_Scaled + Normalized_Net_Income_M_Scaled + Debt_to_Equity_Scaled + Fed.Funds.Rate*Unemployment.Rate) + (Period.Numeric*(Revenue_M_Scaled + Normalized_Net_Income_M_Scaled + Debt_to_Equity_Scaled + Fed.Funds.Rate*Unemployment.Rate)|Company.Name), REML=T, data= fulldf)

# summary(model.h)

# This would not fit, infinite loading

```
```{r}
# Model H - dropping BV/share

model.h <- lmer(Last.Sale.Price.Change.From.Baseline.... ~ Period.Numeric + Revenue_M_Scaled + Normalized_Net_Income_M_Scaled + Debt_to_Equity_Scaled + Fed.Funds.Rate + Unemployment.Rate + (Period.Numeric + Revenue_M_Scaled + Normalized_Net_Income_M_Scaled + Debt_to_Equity_Scaled + Fed.Funds.Rate + Unemployment.Rate|Company.Name), REML=T, data= fulldf)

summary(model.h)


```
```{r}
anova(model.f, model.h)

```
model f remains superior

```{r}
# Model I - dropping unemployment rate

model.i <- lmer(Last.Sale.Price.Change.From.Baseline.... ~ Period.Numeric + Revenue_M_Scaled + Normalized_Net_Income_M_Scaled + Debt_to_Equity_Scaled + Fed.Funds.Rate + (Period.Numeric + Revenue_M_Scaled + Normalized_Net_Income_M_Scaled + Debt_to_Equity_Scaled + Fed.Funds.Rate|Company.Name), REML=T, data= fulldf)

summary(model.i)

```
```{r}
anova(model.f, model.i)

```
Model f remains the best and thus is the final model for this study, with an AIC of 89625 and BIC of 89879

```{r}
# Model F Diagnostics


library(lme4)
library(lmerTest)
library(car)
library(influence.ME)


# Residuals Plot

plot(model.f)

# QQ Plotting

re_effects <- ranef(model.f)

# Assuming re_effects is a list obtained from ranef(model.f)
re_effects_comp <- re_effects$Company.Name

# Setup plotting area
par(mfrow=c(3, 3)) # Adjust this if you have more than 9 effects to plot

# Loop over the random effects components for Company.Name
for (effect_name in names(re_effects_comp)) {
  # Plot the QQ plot for the random effect
  qqnorm(re_effects_comp[[effect_name]], main=paste("QQ Plot:", effect_name))
  qqline(re_effects_comp[[effect_name]])
}

# Reset the plotting window
par(mfrow=c(1, 1))


```
Fit looks good, outliers (stocks that either increased a lot or decreased a lot over the past 4 years have a hard time fitting but in general the model is good)

QQ Plots show normal distributions for the random effects but there are outliers especially for revenue and net income (some companies perform extremely well), debt/equity, fed funds rate

```{r}


# Assuming the model is not singular now, run the influence analysis
infl_group <- influence(model.f, group = "Company.Name")

# Infinite loading - says boundary (singular) fit

```
I think we're done here


